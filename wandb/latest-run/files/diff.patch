diff --git a/model/datasets/__pycache__/utils.cpython-38.pyc b/model/datasets/__pycache__/utils.cpython-38.pyc
index 2a2aee6..e853b9a 100644
Binary files a/model/datasets/__pycache__/utils.cpython-38.pyc and b/model/datasets/__pycache__/utils.cpython-38.pyc differ
diff --git a/model/datasets/utils.py b/model/datasets/utils.py
index 8a37ec8..8aea62b 100644
--- a/model/datasets/utils.py
+++ b/model/datasets/utils.py
@@ -97,7 +97,7 @@ def create_test_dataloader(domain_img_test, data_path, im_size, win_size, win_st
 
 def train_function(model, max_epochs, train_dataloader,val_dataloader, n_channels, device,optimizer, loss_fn, accuracy ,scheduler, strategy, step, seed):
     wandb.login(key = "a60322f26edccc6c3f79accc480d56e52e02750a")
-    wandb.init(project=strategy, tags = str(step), name = strategy+str(seed))
+    wandb.init(project=strategy, tags = str(step), name = str(step)+'_'+strategy+'_'+str(seed))
     columns = ['run','step','ep', 'train_loss', 'train_acc','val_acc', 'val_loss','time', 'method']
     start_epoch = 0
     for epoch in range(start_epoch, max_epochs):
@@ -138,31 +138,25 @@ def train_function(model, max_epochs, train_dataloader,val_dataloader, n_channel
         scheduler.step()
         # model.eval()
         for i, batch in enumerate(val_dataloader):
-            while True :
-                try : 
-                    image = (batch['image'][:,:n_channels,:,:]/255.).to(device)
-                    target = (batch['mask']).to(device)  
-                    # domain_ids = [list_of_tuples[dict_of_tuples[element]][1] for element in batch['id']] 
-                    output = model(image)  
-                    loss = loss_fn(output, target)           
-                    batch['preds'] = output
-                    batch['image'] = image
-                    cm = compute_conf_mat(
-                        target.clone().detach().flatten().cpu(),
-                        ((torch.sigmoid(output)>0.5).cpu().long().flatten()).clone().detach(), 2)
-                    metrics_per_class_df, macro_average_metrics_df, micro_average_metrics_df = dl_inf.cm2metrics(cm.numpy()) 
-                    iou += metrics_per_class_df.IoU[1]
-                    precision += metrics_per_class_df.Precision[1] 
-                    recall += metrics_per_class_df.Recall[1]
-                    loss_sum += loss.item()
-                    acc_sum += accuracy(torch.transpose(output,0,1).reshape(2, -1).t(), 
-                                        torch.transpose(target.to(torch.uint8),0,1).reshape(2, -1).t())
-                except Exception  as e:
-                    print(f"Erreur rencontrée lors de l'itération {i + 1}: {str(e)}")
-                    # Relance l'itération en continuant la boucle while
-                    continue
-                else:
-                    break
+           
+            image = (batch['image'][:,:n_channels,:,:]/255.).to(device)
+            target = (batch['mask']).to(device)  
+            # domain_ids = [list_of_tuples[dict_of_tuples[element]][1] for element in batch['id']] 
+            output = model(image)  
+            loss = loss_fn(output, target)           
+            batch['preds'] = output
+            batch['image'] = image
+            cm = compute_conf_mat(
+                target.clone().detach().flatten().cpu(),
+                ((torch.sigmoid(output)>0.5).cpu().long().flatten()).clone().detach(), 2)
+            metrics_per_class_df, macro_average_metrics_df, micro_average_metrics_df = dl_inf.cm2metrics(cm.numpy()) 
+            iou += metrics_per_class_df.IoU[1]
+            precision += metrics_per_class_df.Precision[1] 
+            recall += metrics_per_class_df.Recall[1]
+            loss_sum += loss.item()
+            acc_sum += accuracy(torch.transpose(output,0,1).reshape(2, -1).t(), 
+                                torch.transpose(target.to(torch.uint8),0,1).reshape(2, -1).t())
+                
         val_iou = {'iou':iou/len(val_dataloader)}
         val_precision = {'prec':precision/len(val_dataloader)}
         val_recall = {'recall':recall/len(val_dataloader)}
